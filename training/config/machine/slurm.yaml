# SLURM cluster configuration for DDP training
name: slurm

# Hardware settings
batch_size: 6  # per GPU
val_batch_size: 1  # per GPU (typically 1 for pose estimation)
num_workers: 8  # per GPU
gpus: 4  # number of GPUs
nodes: 1  # number of nodes

# Root directory (adjust for your cluster)
root_dir: /strg/E/shared-data/Shahaf/gigapose

# Logger backend (tensorboard or wandb)
logger_backend: tensorboard
dryrun: false  # Set to true to disable progress bars and run in offline mode

# Callbacks
callbacks:
  checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    dirpath: ${checkpoint_dir}
    filename: '{epoch:03d}-{val_loss:.4f}'
    save_top_k: 3
    save_last: true
    monitor: val_loss
    mode: min
    every_n_epochs: 1
    save_on_train_epoch_end: true
    verbose: true
  
  lr_monitor:
    _target_: pytorch_lightning.callbacks.LearningRateMonitor
    logging_interval: step

# PyTorch Lightning Trainer settings for DDP
trainer:
  _target_: pytorch_lightning.Trainer
  accelerator: gpu
  devices: ${machine.gpus}
  num_nodes: ${machine.nodes}
  strategy: ddp  # Distributed Data Parallel
  max_epochs: ${max_epochs}
  log_every_n_steps: ${log_every_n_steps}
  val_check_interval: ${val_check_interval}
  precision: 16-mixed
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1
  num_sanity_val_steps: 2
  
  # Logging
  enable_checkpointing: true
  enable_progress_bar: true
  enable_model_summary: true
  
  # DDP settings
  sync_batchnorm: true
  replace_sampler_ddp: true
